#!/usr/bin/env python
# -*- coding: utf-8 -*-
from __future__ import print_function, division
import argparse
import logging
import re
import shlex
import signal
import sys

import gevent
import gevent.socket
import gevent.subprocess
import redis.connection

from reinferio import jobs


logging.basicConfig(format='%(asctime)s - %(levelname)s] %(message)s',
                    datefmt="%Y-%m-%d %H:%M:%S")
log = logging.getLogger()
log.setLevel(logging.INFO)


# Patch redis connections to use gevent sockets.
redis.connection.socket = gevent.socket

DEFAULT_REDIS = 'localhost:6379'
DEFAULT_HEARTBEAT_SECONDS = 60

SLAVE_SUCCESS = 0
SLAVE_FAILURE = 1
SLAVE_PROGRESS = 2


def run_command(commandline, heartbeat_seconds):
    try:
        slave = gevent.subprocess.Popen(commandline,
                                        stdout=gevent.subprocess.PIPE,
                                        stderr=gevent.subprocess.PIPE)
    except OSError, e:
        def generator():
            errmsg = 'popen failed %r - %s' % (commandline, e)
            yield SLAVE_FAILURE, 'ERROR: %s:' % errmsg
            log.fatal(errmsg)
        return generator

    def generator():
        while True:
            line = None
            if heartbeat_seconds <= 0:
                line = slave.stdout.readline()
            else:
                with gevent.Timeout(heartbeat_seconds, False):
                    line = slave.stdout.readline()

            if line is None:
                slave.kill()
                yield SLAVE_FAILURE, \
                    'ERROR task timed out; stderr:\n%s' % \
                    slave.stderr.read()
                return

            if line == '':
                timed_out = False
                with gevent.Timeout(1, False):
                    errc = slave.wait()
                    if errc == 0:
                        yield SLAVE_SUCCESS, None
                    elif errc < 0:
                        yield SLAVE_FAILURE, 'ERROR signal %d; stderr:\n%s' % \
                            (-errc, slave.stderr.read())
                    else:
                        yield SLAVE_FAILURE, \
                            'ERROR exit with %d; stderr:\n%s' % \
                            (errc, slave.stderr.read())
                if timed_out:
                    slave.kill()
                    yield SLAVE_FAILURE, 'ERROR: Interleaved timeout.'
                return
            yield SLAVE_PROGRESS, line[:-1]

    return generator


def worker(job_queue, job_type, queue_options, command):
    log.info('Worker for %s up.', job_type)
    heartbeat = queue_options['heartbeat_secs']
    while True:
        try:
            job_id = job_queue.pop(job_type)
        except gevent.GreenletExit:
            return

        job_meta = job_queue.fetch_snapshot(job_id)
        job_queue.publish_progress(job_id)
        log.info('worker: Started %s/%s.', job_type, job_id)
        for msg, text in run_command(command + job_meta.args, heartbeat)():
            if msg == SLAVE_SUCCESS:
                job_queue.resolve(job_id)
                log.info('worker: Success %s/%s.', job_type, job_id)
            elif msg == SLAVE_FAILURE:
                job_queue.fail(job_id, text)
                log.info('worker: Failure %s/%s: %s', job_type, job_id, text)
            else:
                job_queue.publish_progress(job_id, text)
                log.info('worker: Progress %s/%s: %s', job_type, job_id, text)


def signal_handler(job_queue, greenlets):
    job_queue._redis.connection_pool.disconnect()
    for g in greenlets:
        g.kill()
    log.info('signal_handler: SIGINT caught, terminating greenlets...')


def parse_queue_options(options, options_string):
    """ Parses a 'option1=val1, option2=val2 ...' and updates the 'options'
    dictionary with the new values."""

    if not options_string:
        return options

    splitter = shlex.shlex(options_string, posix=True)
    splitter.whitespace += ','
    splitter.whitespace_split = True

    for (option, value) in (tuple(s.split('=', 1)) for s in splitter):
        try:
            options[option] = type(options[option])(value)
        except ValueError:
            print("invalid queue option value '%s' for '%s'" % (value, option))
            sys.exit(1)
        except KeyError:
            print('unknown queue option \'%s\'' % option)
            print('valid options are: %s' % ' '.join(options.iterkeys()))
            sys.exit(1)

    return options


def parse_mappings(job_queue, mappings):
    regex = re.compile(r'^(?:(\d+)@)?([a-zA-Z_]\w*)(?:\[([^\]]*)\])?:(.+)$')
    workers = []

    def add_n_workers(n, job_type, queue_options, command):
        for _ in xrange(n):
            workers.append(
                lambda: worker(job_queue, job_type, queue_options, command))

    for i_match, match in enumerate(map(regex.match, mappings)):
        if not match:
            print("invalid mapping '%s'" % args.mapping[i_match])
            print('note: format is [NWORKERS@]JOBTYPE[[OPTIONS]]:COMMAND and'
                  ' JOBTYPE is an alphanumeric (and \'_\') identifier which'
                  ' must not begin with a digit.')
            sys.exit(1)

        (n_workers, job_type, options_string, command) = match.groups()

        n_workers = int(n_workers) if n_workers else 1
        command = shlex.split(command)
        queue_options = {'heartbeat_secs': DEFAULT_HEARTBEAT_SECONDS}
        queue_options = parse_queue_options(queue_options, options_string)

        log.info('main: Creating %d worker(s) for %s, with command %r and'
                 ' options %s.', n_workers, job_type, command, queue_options)

        add_n_workers(n_workers, job_type, queue_options, command)

    return workers


def parse_hostport(hostport):
    RX = r'^(?P<host>[A-Za-z0-9-_.]+):(?P<port>[0-9]+)$'
    match = re.match(RX, hostport)
    if not match:
        log.error('string "%s" is not of form host:port' % hostport)
        sys.exit(1)
    groups = match.groupdict()
    groups['port'] = int(groups['port'])
    return groups


if __name__ == '__main__':
    parser = argparse.ArgumentParser(
        description='Rapture is a language agnostic distributed task queue '
        'built on top of Redis and written in Python.')
    _arg = parser.add_argument
    _arg('--redis', metavar='host:port', type=str, default=DEFAULT_REDIS,
         action='store', help='Redis endpoint specified as host:port - '
         'default: %s' % DEFAULT_REDIS)
    _arg('mapping', metavar='num@jobtype:command', type=str, nargs='+',
         help='Job specification - can be specified multiple times.\n'
         'For example: 4@parse:/usr/bin/parser would run '
         'the command /usr/bin/parser up to a maximum of 4 processes '
         'in parallel with the job type identified as "parse"')
    args = parser.parse_args()

    job_queue = jobs.connect_to_queue(**parse_hostport(args.redis))

    workers = map(gevent.spawn, parse_mappings(job_queue, args.mapping))
    gevent.signal(signal.SIGINT, lambda: signal_handler(job_queue, workers))
    log.info('main: Spawned %d greenlets. Waiting on jobs...', len(workers))
    gevent.joinall(workers)

    log.info('main: Clean exit.')
